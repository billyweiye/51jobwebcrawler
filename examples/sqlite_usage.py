#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SQLiteä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•åœ¨ä»£ç ä¸­ä½¿ç”¨SQLiteæ•°æ®åº“è¿›è¡Œæœ¬åœ°å¼€å‘å’Œæµ‹è¯•
"""

import os
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils import (
    create_sqlite_database_manager,
    create_database_manager_from_config,
    ConfigManager
)
from crawler import JobSpider, create_job_spider_from_config


def example_1_basic_sqlite_usage():
    """ç¤ºä¾‹1: åŸºç¡€SQLiteä½¿ç”¨"""
    print("=== ç¤ºä¾‹1: åŸºç¡€SQLiteä½¿ç”¨ ===")
    
    # åˆ›å»ºSQLiteæ•°æ®åº“ç®¡ç†å™¨
    db_manager = create_sqlite_database_manager('./data/example1.db')
    
    try:
        # æ’å…¥æµ‹è¯•æ•°æ®
        job_data = {
            'job_id': 'example_001',
            'title': 'Pythonåç«¯å¼€å‘å·¥ç¨‹å¸ˆ',
            'company_name': 'ç¤ºä¾‹ç§‘æŠ€æœ‰é™å…¬å¸',
            'salary_min': 15000,
            'salary_max': 25000,
            'salary_text': '15k-25k',
            'location_city': 'ä¸Šæµ·',
            'location_district': 'æµ¦ä¸œæ–°åŒº',
            'job_description': 'è´Ÿè´£åç«¯ç³»ç»Ÿå¼€å‘ï¼Œç†Ÿæ‚‰Pythonã€Djangoç­‰æŠ€æœ¯æ ˆ',
            'experience_required': '3-5å¹´',
            'education_required': 'æœ¬ç§‘',
            'industry': 'ITäº’è”ç½‘',
            'publish_time': datetime.now(),
            'job_url': 'https://jobs.51job.com/example/001.html'
        }
        
        # æ’å…¥æ•°æ®
        success = db_manager.insert_job_listing(job_data)
        print(f"æ•°æ®æ’å…¥ç»“æœ: {success}")
        
        # æŸ¥è¯¢æ•°æ®
        jobs = db_manager.search_job_listings(
            keywords=['Python'],
            cities=['ä¸Šæµ·'],
            limit=10
        )
        print(f"æŸ¥è¯¢åˆ° {len(jobs)} æ¡èŒä½")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = db_manager.get_statistics()
        print(f"æ•°æ®åº“ç»Ÿè®¡: æ€»èŒä½æ•° {stats['total_jobs']}")
        
    finally:
        db_manager.close()


def example_2_config_based_sqlite():
    """ç¤ºä¾‹2: åŸºäºé…ç½®çš„SQLiteä½¿ç”¨"""
    print("\n=== ç¤ºä¾‹2: åŸºäºé…ç½®çš„SQLiteä½¿ç”¨ ===")
    
    # åˆ›å»ºé…ç½®ç®¡ç†å™¨
    config_manager = ConfigManager()
    
    # æ£€æŸ¥SQLiteæ˜¯å¦å·²å¯ç”¨
    db_config = config_manager.get_database_config()
    sqlite_enabled = db_config.get('sqlite', {}).get('enabled', False)
    
    print(f"SQLiteé…ç½®çŠ¶æ€: {'å·²å¯ç”¨' if sqlite_enabled else 'æœªå¯ç”¨'}")
    
    if sqlite_enabled:
        # ä½¿ç”¨é…ç½®åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨
        db_manager = create_database_manager_from_config(config_manager)
        
        try:
            # æµ‹è¯•è¿æ¥
            if db_manager.test_connection():
                print("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
                
                # æ‰¹é‡æ’å…¥ç¤ºä¾‹æ•°æ®
                batch_data = [
                    {
                        'job_id': f'batch_{i:03d}',
                        'title': f'èŒä½æ ‡é¢˜ {i}',
                        'company_name': f'å…¬å¸ {i}',
                        'salary_text': f'{10+i}k-{15+i}k',
                        'location_city': 'åŒ—äº¬' if i % 2 == 0 else 'ä¸Šæµ·'
                    }
                    for i in range(1, 6)
                ]
                
                inserted, skipped = db_manager.batch_insert_job_listings(batch_data)
                print(f"æ‰¹é‡æ’å…¥ç»“æœ: æˆåŠŸ {inserted}, è·³è¿‡ {skipped}")
                
            else:
                print("âœ— æ•°æ®åº“è¿æ¥å¤±è´¥")
                
        finally:
            db_manager.close()
    else:
        print("è¯·å…ˆè¿è¡Œ 'python setup_sqlite.py' å¯ç”¨SQLiteé…ç½®")


def example_3_crawler_with_sqlite():
    """ç¤ºä¾‹3: çˆ¬è™«ä¸SQLiteé›†æˆ"""
    print("\n=== ç¤ºä¾‹3: çˆ¬è™«ä¸SQLiteé›†æˆ ===")
    
    try:
        # åˆ›å»ºé…ç½®ç®¡ç†å™¨
        config_manager = ConfigManager()
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        spider = create_job_spider_from_config(config_manager)
        
        print("âœ“ çˆ¬è™«åˆ›å»ºæˆåŠŸ")
        
        # æ‰§è¡Œå°è§„æ¨¡æµ‹è¯•çˆ¬å–
        with spider:
            print("å¼€å§‹æµ‹è¯•çˆ¬å–...")
            
            # çˆ¬å–å°‘é‡æ•°æ®è¿›è¡Œæµ‹è¯•
            results = spider.crawl_jobs(
                keywords=['Python'],
                cities=['ä¸Šæµ·'],
                max_pages=1,  # åªçˆ¬å–1é¡µ
                delay_range=(1, 2)
            )
            
            print(f"âœ“ çˆ¬å–å®Œæˆ: {len(results)} æ¡æ•°æ®")
            
            # æ˜¾ç¤ºçˆ¬è™«ç»Ÿè®¡
            stats = spider.get_stats()
            print(f"çˆ¬è™«ç»Ÿè®¡: æˆåŠŸ {stats.successful_requests}, å¤±è´¥ {stats.failed_requests}")
            
            # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡
            db_stats = spider.db_manager.get_statistics()
            print(f"æ•°æ®åº“ç»Ÿè®¡: æ€»èŒä½ {db_stats['total_jobs']}")
            
    except Exception as e:
        print(f"âœ— çˆ¬è™«æµ‹è¯•å¤±è´¥: {e}")


def example_4_data_export_import():
    """ç¤ºä¾‹4: æ•°æ®å¯¼å‡ºå¯¼å…¥"""
    print("\n=== ç¤ºä¾‹4: æ•°æ®å¯¼å‡ºå¯¼å…¥ ===")
    
    db_manager = create_sqlite_database_manager('./data/example4.db')
    
    try:
        # å¯¼å‡ºæ•°æ®åˆ°DataFrame
        df = db_manager.export_to_dataframe(
            query="SELECT * FROM job_listings LIMIT 10"
        )
        print(f"å¯¼å‡ºæ•°æ®: {len(df)} è¡Œ")
        
        if not df.empty:
            # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
            print("\næ•°æ®æ¦‚è§ˆ:")
            print(df[['job_id', 'title', 'company_name', 'salary_text']].head())
            
            # ä¿å­˜åˆ°CSVæ–‡ä»¶
            csv_file = './data/exported_jobs.csv'
            df.to_csv(csv_file, index=False, encoding='utf-8')
            print(f"âœ“ æ•°æ®å·²å¯¼å‡ºåˆ°: {csv_file}")
        
    except Exception as e:
        print(f"âœ— æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
    finally:
        db_manager.close()


def example_5_database_maintenance():
    """ç¤ºä¾‹5: æ•°æ®åº“ç»´æŠ¤"""
    print("\n=== ç¤ºä¾‹5: æ•°æ®åº“ç»´æŠ¤ ===")
    
    db_manager = create_sqlite_database_manager('./data/example5.db')
    
    try:
        # è·å–æ•°æ®åº“ç»Ÿè®¡
        stats = db_manager.get_statistics()
        print(f"å½“å‰æ•°æ®åº“çŠ¶æ€:")
        print(f"  æ€»èŒä½æ•°: {stats['total_jobs']}")
        print(f"  æŸ¥è¯¢æˆåŠŸç‡: {stats['database_stats'].success_rate:.2%}")
        
        # æ¸…ç†æ—§æ•°æ®ï¼ˆä¿ç•™æœ€è¿‘30å¤©ï¼‰
        deleted_count = db_manager.cleanup_old_data(days=30)
        print(f"âœ“ æ¸…ç†æ—§æ•°æ®: åˆ é™¤ {deleted_count} æ¡è®°å½•")
        
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        db_manager.reset_stats()
        print("âœ“ ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
        
    except Exception as e:
        print(f"âœ— æ•°æ®åº“ç»´æŠ¤å¤±è´¥: {e}")
    finally:
        db_manager.close()


def main():
    """ä¸»å‡½æ•°"""
    print("SQLiteä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    os.makedirs('./data', exist_ok=True)
    
    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    example_1_basic_sqlite_usage()
    example_2_config_based_sqlite()
    example_3_crawler_with_sqlite()
    example_4_data_export_import()
    example_5_database_maintenance()
    
    print("\n=== æ‰€æœ‰ç¤ºä¾‹æ‰§è¡Œå®Œæˆ ===")
    print("\nğŸ’¡ æç¤º:")
    print("  - SQLiteæ•°æ®åº“æ–‡ä»¶ä¿å­˜åœ¨ ./data/ ç›®å½•ä¸‹")
    print("  - å¯ä»¥ä½¿ç”¨SQLiteæµè§ˆå™¨å·¥å…·æŸ¥çœ‹æ•°æ®åº“å†…å®¹")
    print("  - ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨MySQLæ•°æ®åº“")


if __name__ == '__main__':
    main()